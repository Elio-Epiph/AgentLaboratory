#!/usr/bin/env python3
"""
CPU æ¨ç†ä¼˜åŒ–è„šæœ¬
ç”¨äºæé«˜æœ¬åœ°æ¨¡å‹åœ¨ CPU ä¸Šçš„æ¨ç†é€Ÿåº¦
"""

import torch
import os
import sys
from transformers import AutoModelForCausalLM, AutoTokenizer
import time

def optimize_for_cpu_inference(model_path):
    """
    ä¼˜åŒ–æ¨¡å‹ç”¨äº CPU æ¨ç†
    """
    print(f"Optimizing model for CPU inference: {model_path}")
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        print(f"âŒ Model path does not exist: {model_path}")
        return None
    
    try:
        # 1. åŠ è½½ tokenizer
        print("Loading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        
        # 2. åŠ è½½æ¨¡å‹å¹¶ä¼˜åŒ–
        print("Loading and optimizing model...")
        
        # CPU ä¼˜åŒ–é€‰é¡¹
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float32,  # CPU ä¸Šç”¨ float32
            device_map="cpu",
            low_cpu_mem_usage=True,     # å‡å°‘å†…å­˜ä½¿ç”¨
            trust_remote_code=True
        )
        
        # 3. è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        model.eval()
        
        # 4. å°è¯•è¿›ä¸€æ­¥ä¼˜åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        try:
            # ä½¿ç”¨ torch.compile ä¼˜åŒ–ï¼ˆPyTorch 2.0+ï¼‰
            if hasattr(torch, 'compile'):
                print("Applying torch.compile optimization...")
                model = torch.compile(model, mode="reduce-overhead")
        except Exception as e:
            print(f"âš ï¸  torch.compile not available: {e}")
        
        print("âœ… Model optimized for CPU inference!")
        return tokenizer, model
        
    except Exception as e:
        print(f"âŒ Error optimizing model: {e}")
        return None

def benchmark_inference(tokenizer, model, prompt, num_runs=5):
    """
    æµ‹è¯•æ¨ç†é€Ÿåº¦
    """
    print(f"\nBenchmarking inference speed ({num_runs} runs)...")
    
    # å‡†å¤‡è¾“å…¥
    full_prompt = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
    inputs = tokenizer(full_prompt, return_tensors="pt")
    
    times = []
    
    for i in range(num_runs):
        start_time = time.time()
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=100,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        end_time = time.time()
        inference_time = end_time - start_time
        times.append(inference_time)
        
        print(f"Run {i+1}: {inference_time:.2f}s")
    
    avg_time = sum(times) / len(times)
    print(f"\nAverage inference time: {avg_time:.2f}s")
    print(f"Tokens per second: {100/avg_time:.1f}")
    
    return avg_time

def get_memory_usage():
    """
    è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ
    """
    import psutil
    process = psutil.Process(os.getpid())
    memory_mb = process.memory_info().rss / 1024 / 1024
    print(f"Memory usage: {memory_mb:.1f} MB")
    return memory_mb

def main():
    """
    ä¸»å‡½æ•°
    """
    print("=" * 60)
    print("CPU Inference Optimization for Local Models")
    print("=" * 60)
    
    # æ¨¡å‹è·¯å¾„
    model_path = "/data/kaohesheng/qiujinbo/Qwen2.5-1.5B-Instruct"
    
    # æ£€æŸ¥ PyTorch ç‰ˆæœ¬
    print(f"PyTorch version: {torch.__version__}")
    print(f"CUDA available: {torch.cuda.is_available()}")
    
    # ä¼˜åŒ–æ¨¡å‹
    result = optimize_for_cpu_inference(model_path)
    if result is None:
        return
    
    tokenizer, model = result
    
    # æµ‹è¯•æ¨ç†é€Ÿåº¦
    test_prompt = "Explain what is machine learning in one sentence."
    benchmark_inference(tokenizer, model, test_prompt)
    
    # æ£€æŸ¥å†…å­˜ä½¿ç”¨
    get_memory_usage()
    
    print("\nğŸ‰ CPU optimization completed!")
    print("\nTips for faster CPU inference:")
    print("1. Use smaller models (1.5B vs 7B)")
    print("2. Reduce max_new_tokens")
    print("3. Use lower temperature for deterministic output")
    print("4. Consider using int8 quantization if memory allows")

if __name__ == "__main__":
    main() 