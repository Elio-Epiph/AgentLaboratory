# If you want to have user input or be a human-in-the-loop
copilot-mode: False

# Here is the research prompt. If num-papers-to-write > 1, you can treat this as a "research direction" otherwise it can be *very* specific and can be treated as a full research idea
research-topic: "Analyze the performance of large language models on sentiment classification tasks using the IMDb datasets."

# Local model configuration (no API key needed)
# api-key: "sk-zk2ea9cda550517658186d2f7edd09236c66873c80a55b90"
# or deepseek-api-key: "DEEPSEEK-API-KEY-HERE"
# Agent Laboratory backend - using local Qwen model
llm-backend: "qwen-local:/data/kaohesheng/qiujinbo/Qwen2.5-7B-Instruct"
# Literature review backend - using local Qwen model
lit-review-backend: "qwen-local:/data/kaohesheng/qiujinbo/Qwen2.5-7B-Instruct"

# Base language
language: "English"

# Number of arxiv papers to lit review
num-papers-lit-review: 5
# Total number of papers to write in sequence
num-papers-to-write: 1
# Do you want to run multiple agent labs in parallel?
parallel-labs: False

# Total mle-solver steps per lab
mlesolver-max-steps: 3
# Total paper-solver steps per lab
papersolver-max-steps: 1
# The lab index for this lab (used for parallel runs)
lab-index: 1
# If you want to load an existing save
load-previous: False
# If fail, run exception?
except-if-fail: False
# Compile latex into PDFs during paper-solver
compile-latex: False

# Task notes
task-notes:
  plan-formulation:
    - 'Design a simple prompting technique to perform sentiment classification on movie reviews.'
    - 'Use Qwen2.5-7B-Instruct as the language model.'
    - 'Your goal is not to beat the SOTA but to demonstrate successful execution of the pipeline.'
    - 'Use only 10 randomly selected samples from the IMDb test set (seed=42).'
    - 'Do not over-optimize the prompt â€“ a single clear and natural instruction in English is sufficient.'
    - "When performing the literature review, only select papers that are highly relevant to the research topic. For example, if the research topic is 'Analyze the performance of large language models on sentiment classification tasks using the IMDb datasets', you should select papers focusing on sentiment classification tasks using LLMs rather than only examing its robustness or something else. Note that the important thing is to focus on classification tasks."
    - "For each paper you add, first output a brief explanation (1-2 sentences) of why this paper is directly relevant to the research topic."
    - "Do not add papers that are only loosely related or focus on other tasks, datasets, or domains."
  data-preparation:
    - 'Use the IMDb dataset from HuggingFace. Sample 10 test examples with seed=42.'
    - 'You may use the following code:\nfrom datasets import load_dataset\nimport random\nrandom.seed(42)\nimdb = load_dataset("imdb")["test"]\nsample = random.sample(list(imdb), 10)'
    - 'Truncate long reviews to a maximum of 512 characters for efficiency.'
  running-experiments:
    - 'Use query_qwen_local(prompt=..., system_prompt=..., model_path=...) to get the Qwen model response.'
    - 'Wrap each review in a prompt asking the model to classify its sentiment as positive or negative.'
    - 'Use multiprocessing to parallelize 10-sample inference.'
    - 'Compare model predictions to ground truth labels using simple string or label matching.'
  results-interpretation:
    - 'Report how many predictions were correct out of 10.'
    - 'Optionally print each review + model prediction + ground truth label.'
  report-writing:
    - 'Summarize the experiment and include the final classification accuracy.'
    - 'Explain briefly the prompting strategy used for sentiment classification.'
    - 'You may include a simple bar chart or table to present results.'
